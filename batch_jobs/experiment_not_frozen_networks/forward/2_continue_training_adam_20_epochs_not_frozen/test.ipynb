{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8c63a3-a094-42c1-b532-c29503d8de1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 17:30:47.343652: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-20 17:30:53.457124: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-20 17:31:12.063701: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bwhpc/common/devel/cuda/11.8/lib64\n",
      "2023-01-20 17:31:12.070792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/bwhpc/common/devel/cuda/11.8/lib64\n",
      "2023-01-20 17:31:12.070814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/scratch/slurm_tmpdir/job_21672698/ipykernel_128754/2235643361.py:14: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from skimage import io\n",
    "from skimage.color import rgb2gray\n",
    "#from skimage.color import rgb2gray, rgb2grey\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from tensorflow.nn import local_response_normalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,BatchNormalization, Lambda, GlobalAveragePooling2D, Dense, Dropout, Flatten, Add, UpSampling3D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow import keras \n",
    "from skimage.util import random_noise\n",
    "import imageio\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "#from numba import cuda \n",
    "import os\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/opt/bwhpc/common/devel/cuda/11.8\"\n",
    "\n",
    "def append_to_file(file_name, val):\n",
    "  f = open(file_name, \"a\")\n",
    "  f.write(\"{0}\\n\".format(str(val)))\n",
    "  f.close()\n",
    "\n",
    "def gaussian_noise(image):\n",
    "    row,col,ch= image.shape\n",
    "    mean = 0\n",
    "    var = 0.01\n",
    "    sigma = var**0.4\n",
    "    gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "    gauss = gauss.reshape(row,col,ch)\n",
    "    noisy = image + gauss\n",
    "    return noisy\n",
    "\n",
    "\n",
    "def tf_gaussian_noise(x, y):\n",
    "    x = tf.py_function(gaussian_noise, [x], np.float32)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def salt_pepper_noise(img):\n",
    "    pad = 150\n",
    "    show = 1\n",
    "    noise = np.random.randint(pad, size = (img.shape[0], img.shape[1], 3))\n",
    "    img = np.where(noise == 0, 0, img)\n",
    "    img = np.where(noise == (pad-1), 1, img)\n",
    "    noise = noise / 255\n",
    "    return noise + img\n",
    "\n",
    "\n",
    "def tf_salt_pepper_noise(x, y):\n",
    "    x = tf.py_function(salt_pepper_noise, [x], np.float32)\n",
    "    return x, y\n",
    "\n",
    "class FeedforwardModel(keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(FeedforwardModel, self).__init__(name=\"FFmodel\", **kwargs)\n",
    "    VGG = VGG16(weights='imagenet', include_top=False)\n",
    "    self.VGG_without_maxpooling = keras.Model(VGG.input, VGG.layers[-2].output)\n",
    "    self.VGG_without_maxpooling._name = \"not_frozen_vgg16\"\n",
    "    self.flatten = Flatten()\n",
    "    self.dense = Dense(256, activation= 'relu')\n",
    "    self.dropout = Dropout(0.5)\n",
    "    self.maxpooling = MaxPooling2D(pool_size=(2,2), strides= (2,2))\n",
    "    self.output_layer = Dense(num_classes, activation='softmax')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.VGG_without_maxpooling(inputs)\n",
    "    x = self.maxpooling(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense(x) \n",
    "    x = self.dropout(x)\n",
    "    x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a96a64c3-83d9-42f6-8861-1f0d2831f3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "path_to_datasets = '../../../../create_datasets/normalize_dataset/saved_datasets/'\n",
    "train_ds_prepared_without_batch = tf.data.Dataset.load(os.path.join(path_to_datasets, \"train_ds_without_batching\"))\n",
    "test_ds_prepared_without_batch = tf.data.Dataset.load(os.path.join(path_to_datasets, \"test_ds_without_batching\"))\n",
    "valid_ds_prepared_without_batch = tf.data.Dataset.load(os.path.join(path_to_datasets, \"valid_ds_without_batching\"))\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "\n",
    "batch_size = 1\n",
    "num_classes = 16\n",
    "img_size = 224\n",
    "input_shape = (None, img_size, img_size, 3) \n",
    "epochs= 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b387622-81a0-4e71-8bc7-495c65f90859",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    train_ds = train_ds_prepared_without_batch.shuffle(10000).batch(batch_size).prefetch(4)\n",
    "    valid_ds = valid_ds_prepared_without_batch.batch(batch_size).prefetch(4)\n",
    "    test_ds = test_ds_prepared_without_batch.batch(batch_size).prefetch(4)\n",
    "    test_ds_gaussian_noise = test_ds_prepared_without_batch.map(tf_gaussian_noise).batch(batch_size).prefetch(4)\n",
    "    test_ds_salt_pepper_noise = test_ds_prepared_without_batch.map(tf_salt_pepper_noise).batch(batch_size).prefetch(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a8788b-758f-49b7-b75a-3f4fa726a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '../1_train_10_epochs_frozen/tmp/checkpoint'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531875da-12a8-4fe0-8dac-d70fb856be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FFmodel\"\n",
      "____________________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   Trainable  \n",
      "============================================================================\n",
      " not_frozen_vgg16 (Functiona  (None, None, None, 512)  14714688  Y          \n",
      " l)                                                                         \n",
      "                                                                            \n",
      " flatten_1 (Flatten)         multiple                  0         Y          \n",
      "                                                                            \n",
      " dense_2 (Dense)             multiple                  6422784   Y          \n",
      "                                                                            \n",
      " dropout_1 (Dropout)         multiple                  0         Y          \n",
      "                                                                            \n",
      " max_pooling2d_1 (MaxPooling  multiple                 0         Y          \n",
      " 2D)                                                                        \n",
      "                                                                            \n",
      " dense_3 (Dense)             multiple                  4112      Y          \n",
      "                                                                            \n",
      "============================================================================\n",
      "Total params: 21,141,584\n",
      "Trainable params: 21,141,584\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    sample = next(iter(test_ds))[0]\n",
    "    model = FeedforwardModel()\n",
    "    model.build(input_shape)\n",
    "    model(sample)\n",
    "    model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58b6eac-49d8-4b2a-859a-d561edbceaff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Weights may only be loaded based on topology into Models when loading TensorFlow-formatted weights (got by_name=True to load_weights).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_ds, epochs\u001b[38;5;241m=\u001b[39mepochs, validation_data\u001b[38;5;241m=\u001b[39mvalid_ds, callbacks\u001b[38;5;241m=\u001b[39m[model_checkpoint_callback], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n\u001b[1;32m      6\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-01-02/lib/python3.8/site-packages/keras/engine/training.py:3001\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2999\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint\u001b[38;5;241m.\u001b[39mread(filepath, options)\n\u001b[1;32m   3000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m by_name:\n\u001b[0;32m-> 3001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   3002\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights may only be loaded based on topology into Models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading TensorFlow-formatted weights \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3004\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got by_name=True to load_weights).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3005\u001b[0m     )\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m   3007\u001b[0m     session \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_session()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Weights may only be loaded based on topology into Models when loading TensorFlow-formatted weights (got by_name=True to load_weights)."
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "    start_time = time.time()\n",
    "    model.load_weights(checkpoint_filepath, by_name = True, skip_mismatch=True)\n",
    "    history = model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=[model_checkpoint_callback], verbose=1) \n",
    "    execution_time = time.time() - start_time\n",
    "    results = model.evaluate(test_ds, batch_size=batch_size)\n",
    "    results_gaussian_noise = model.evaluate(test_ds_gaussian_noise, batch_size=batch_size)\n",
    "    result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc8cb0-ce72-4693-8181-68a326e11db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_res_forward = os.path.join(\"./results_forward_model.txt\")\n",
    "if os.path.exists(file_res_forward):\n",
    "  os.remove(file_res_forward)\n",
    "f = open(file_res_forward, \"x\")\n",
    "for v in [\"test loss\", results[0], \"test acc\", results[1],\\\n",
    "         \"test loss gaussian\", results_gaussian_noise[0], \"test acc gaussian\", results_gaussian_noise[1],\\\n",
    "         \"test loss salt_pepper\", results_salt_pepper_noise[0], \"test acc salt_pepper\", results_salt_pepper_noise[1]]:\n",
    "    append_to_file(file_res_forward, v)    \n",
    "\n",
    "history_dict = history.history\n",
    "# Save it under the form of a json file\n",
    "history_path = \"./history_forward.json\"\n",
    "if os.path.exists(history_path):\n",
    "  os.remove(history_path)\n",
    "f = open(history_path, \"x\")\n",
    "json.dump("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b852e1-0436-41aa-8cc8-65dd2359c1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6277a-a0b9-4da3-9d75-b604d172be8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfds2)",
   "language": "python",
   "name": "tfds2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
